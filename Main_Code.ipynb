{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Main_Code.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cdQcaqJIokPm","colab_type":"code","outputId":"5ea15d9f-38f7-4f55-cef1-43e80d8a4ee7","executionInfo":{"status":"ok","timestamp":1558093622006,"user_tz":-120,"elapsed":75046,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":1142}},"source":["!pip install matplotlib\n","!pip install numpy\n","!pip install scipy\n","!pip install sklearn\n","!pip install h5py pyyaml\n","!pip install tf_nightly\n","!pip install tensorboardcolab\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL\n","from scipy.misc import imsave\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","from PIL import Image\n","from statistics import stdev \n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.3)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n","Collecting tf_nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/1d/66b96ef65e4662cdfaf9a9a4fdb99459d4ae0874ca8fa086b4ca5bbfd565/tf_nightly-1.14.1.dev20190517-cp36-cp36m-manylinux1_x86_64.whl (109.3MB)\n","\u001b[K     |████████████████████████████████| 109.3MB 125kB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.0.7)\n","Collecting wrapt>=1.11.1 (from tf_nightly)\n","  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n","Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf_nightly)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/50/35c33d833f79f8b7b8c21a8e5414487adea342b8dfa21f2e20190cb2c578/tb_nightly-1.14.0a20190517-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 36.1MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.0.9)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.33.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.15.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.2.2)\n","Collecting tf-estimator-nightly (from tf_nightly)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/77/22c9fdcedf8c01c740bffa6fd8a526081ec9cffcf8bbdb22c8694a505ec4/tf_estimator_nightly-1.14.0.dev2019051701-py2.py3-none-any.whl (487kB)\n","\u001b[K     |████████████████████████████████| 491kB 42.1MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (3.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.7.1)\n","Collecting google-pasta>=0.1.6 (from tf_nightly)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 21.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.16.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf_nightly) (2.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf_nightly) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf_nightly) (0.15.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf_nightly) (41.0.1)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n","Successfully built wrapt\n","\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n","Installing collected packages: wrapt, tb-nightly, tf-estimator-nightly, google-pasta, tf-nightly\n","  Found existing installation: wrapt 1.10.11\n","    Uninstalling wrapt-1.10.11:\n","      Successfully uninstalled wrapt-1.10.11\n","Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190517 tf-estimator-nightly-1.14.0.dev2019051701 tf-nightly-1.14.1.dev20190517 wrapt-1.11.1\n","Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O6OfBmemoxp3","colab_type":"code","colab":{}},"source":["#Splitting of the data\n","\n","#Training data\n","Xtr = np.ndarray((198,128,128,1))\n","Ytr = np.ndarray((198,128,128,4)) #to comment into 1hot encoding\n","\n","#Validation Data\n","Xva = np.ndarray((25,128,128,1))\n","Yva = np.ndarray((25,128,128,4))\n","#Testing Data\n","Xtest = np.ndarray((22,128,128,1))\n","Ytest = np.ndarray((22,128,128,4))\n","for i in range (1,154):\n","  if i <10:\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN00%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN00%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    #mask = np.reshape(np.array(mask),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    Xtr[i][:][:][:] = im\n","    Ytr[i][:][:][:] = mask\n","  \n","  elif (i>=10) and (i<100):\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN0%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN0%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    Xtr[i][:][:][:] = im\n","    Ytr[i][:][:][:] = mask\n","    \n","  elif (i>=100):\n","    if (i == 122):\n","      continue;\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    Xtr[i][:][:][:] = im\n","    Ytr[i][:][:][:] = mask\n","    \n","    \n","for i in range(1,94):   \n","  if i <10:\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCNN00%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCNN00%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    Xtr[i][:][:][:] = im\n","    Ytr[i][:][:][:] = mask\n","    \n","  elif (i>=10):\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCNN0%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCNN0%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    if i<47:\n","      Xtr[i][:][:][:] = im\n","      Ytr[i][:][:][:] = mask\n","    elif 46<i<72:\n","      Xva[i-47][:][:][:] = im\n","      Yva[i-47][:][:][:] = mask\n","    elif 71<i<94:\n","      Xtest[i-72][:][:][:] = im\n","      Ytest[i-72][:][:][:] = mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pxa3G5xFEJ23","colab_type":"code","outputId":"769bdc5b-2065-41df-fd2b-2a93595c3a75","executionInfo":{"status":"ok","timestamp":1558093921462,"user_tz":-120,"elapsed":1877,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#Osheen Part\n","#Splitting the data randomly \n","from sklearn.model_selection import train_test_split\n","\n","\n","#Splitting of the data\n","\n","X = np.ndarray((246,128,128,1))\n","Y = np.ndarray((246,128,128,4))\n","\n","for i in range (1,154):\n","  if i <10:\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN00%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN00%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    X[i][:][:][:] = im\n","    Y[i][:][:][:] = mask\n","  \n","  elif (i>=10) and (i<100):\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN0%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN0%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    X[i][:][:][:] = im\n","    Y[i][:][:][:] = mask\n","    \n","  elif (i>=100):\n","    if (i == 122):\n","      continue;\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCLN%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCLN%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    X[i][:][:][:] = im\n","    Y[i][:][:][:] = mask\n","    \n","    \n","for i in range(1,94):   \n","  if i <10:\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCNN00%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCNN00%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    X[i][:][:][:] = im\n","    Y[i][:][:][:] = mask\n","    \n","  elif (i>=10):\n","    im = Image.open(\"gdrive/My Drive/Deep Learning Project/Converted_Images_PNG/JPCNN0%s.PNG\" % (i))\n","    mask = Image.open(\"gdrive/My Drive/Deep Learning Project/Merged_Masks_PNG/JPCNN0%s.PNG\" % (i))\n","    im = np.reshape(np.array(im),(128,128,1))\n","    mask = to_categorical(np.array(mask))\n","    if i<47:\n","      X[i][:][:][:] = im\n","      Y[i][:][:][:] = mask\n","    elif 46<i<72:\n","      X[i-47][:][:][:] = im\n","      Y[i-47][:][:][:] = mask\n","    elif 71<i<94:\n","      X[i-72][:][:][:] = im\n","      Y[i-72][:][:][:] = mask\n","    \n","    \n","#Splitting of data into training, validation and testing\n","Xtr, Xtst, Ytr, Ytst = train_test_split(X, Y, test_size=0.10)\n","Xtr, Xva, Ytr, Yva = train_test_split(Xtr, Ytr, test_size=0.10)\n","\n","\n","#Printing shape of Training and Test data\n","print( Xtr.shape, Ytr.shape)\n","print( Xtst.shape, Ytst.shape)\n","print(Xva.shape, Yva.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(198, 128, 128, 1) (198, 128, 128, 4)\n","(25, 128, 128, 1) (25, 128, 128, 4)\n","(23, 128, 128, 1) (23, 128, 128, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"63dY1otR5LNN","colab_type":"code","colab":{}},"source":["#Build the model\n","\n","#Libraries\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Activation, Conv2D, Dropout,MaxPooling2D,UpSampling2D, Input, ELU, Concatenate, BatchNormalization\n","from keras.models import load_model\n","from keras.losses import categorical_crossentropy\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRkqxjFxfIGx","colab_type":"code","outputId":"df33a6ae-d82f-4fe2-f347-b72bf7c536b4","executionInfo":{"status":"ok","timestamp":1558093926371,"user_tz":-120,"elapsed":1297,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["# Model code\n","\n","first_input = Input(shape =(128,128,1))\n","bn = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(first_input)\n","conv1_1 = Conv2D(128, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(bn)\n","conv1_2=Conv2D(128, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv1_1)\n","dropout1 = Dropout(0.1)(conv1_2)\n","maxPool1 = MaxPooling2D(pool_size=2, strides=1)(dropout1)\n","conv2_1 = Conv2D(64, [3,3], strides=(2, 2), activation='elu', padding=\"same\")(maxPool1)\n","conv2_2 = Conv2D(64, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv2_1)\n","dropout2 = Dropout(0.1)(conv2_2)\n","maxPool2 = MaxPooling2D(pool_size=2, strides=1)(dropout2)\n","conv3_1 = Conv2D(32, [3,3], strides=(2, 2), activation='elu', padding=\"same\")(maxPool2)\n","conv3_2 = Conv2D(32, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv3_1)\n","dropout3 = Dropout(0.1)(conv3_2)\n","maxPool3 = MaxPooling2D(pool_size=2, strides=1)(dropout3)\n","\n","conv4_1 = Conv2D(16, [3,3], strides=(2, 2), activation='elu', padding=\"same\")(maxPool3)\n","conv4_2 = Conv2D(16, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv4_1)\n","dropout4 = Dropout(0.1)(conv4_2)\n","\n","upSampling1 = UpSampling2D(size=(2, 2))(dropout4)\n","\n","#concatenate\n","conv_int5 = Conv2D(32, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(upSampling1)\n","concat1 = Concatenate()([dropout3, conv_int5])\n","\n","conv5_1 = Conv2D(32, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(concat1)\n","conv5_2 = Conv2D(32, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv5_1)\n","dropout5 = Dropout(0.1)(conv5_2)\n","upSampling2 = UpSampling2D(size=(2, 2))(dropout5)\n","#concatenate\n","conv_int6 = Conv2D(64, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(upSampling2)\n","concat2 = Concatenate()([dropout2, conv_int6])\n","\n","conv6_1 = Conv2D(64, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(concat2)\n","conv6_2 = Conv2D(64, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv6_1)\n","dropout6 = Dropout(0.1)(conv6_2)\n","upSampling3 = UpSampling2D(size=(2, 2))(dropout6)\n","\n","#concatenate\n","conv_int7 = Conv2D(128, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(upSampling3)\n","concat3 = Concatenate()([dropout1, conv_int7])\n","\n","conv7_1 = Conv2D(128, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(concat3)\n","conv7_2 = Conv2D(128, [3,3], strides=(1, 1), activation='elu', padding=\"same\")(conv7_1)\n","\n","dropout7 = Dropout(0.1)(conv7_2)\n","\n","#batch normalization\n","bn = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(dropout7)\n","\n","output = Conv2D(4, [1,1], strides=(1, 1), activation='softmax', padding=\"same\")(bn)\n","#output2 = Conv2D(4, [1,1], strides=(1, 1), activation='softmax', padding=\"same\")(bn)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0517 11:52:06.282512 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0517 11:52:06.303050 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0517 11:52:06.324307 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0517 11:52:06.325759 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0517 11:52:06.329548 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","W0517 11:52:06.366129 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0517 11:52:06.478714 139964132034432 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0517 11:52:06.498623 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0517 11:52:06.668086 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NvzV0NRccnAG","colab_type":"code","outputId":"4709c17f-5e3b-4bf5-b4f5-c1274bc871e4","executionInfo":{"status":"ok","timestamp":1558093929696,"user_tz":-120,"elapsed":654,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#weighting of the cost function for the different classes\n","print(Ytr.shape)\n","np.set_printoptions(threshold=np.inf)\n","import sklearn\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","Ytr2 = np.argmax(Ytr,3)\n","\n","class0 = Ytr2 == 0\n","size0 = len(np.extract(class0, Ytr2))\n","\n","class1 = Ytr2 == 1\n","size1 = len(np.extract(class1, Ytr2))\n","\n","class2 = Ytr2 == 2\n","size2 = len(np.extract(class2, Ytr2))\n","\n","class3 = Ytr2 == 3\n","size3 = len(np.extract(class3, Ytr2))\n","\n","s = size1 + size2 + size3 + size0\n","\n","weights = {'0' : np.ceil(size0/size0),\n","               '1' : np.ceil(size0/size1),\n","               '2' : np.ceil(size0/size2),\n","               '3' : np.ceil(size0/size3)}\n","\n","#Sample Weights\n","\n","\n","sampleW = Ytr2 \n","sampleW[Ytr2== 1] = np.ceil(size0/size1)\n","sampleW[Ytr2== 0] = np.ceil(size0/size0) \n","sampleW[Ytr2== 2] = np.ceil(size0/size2)\n","sampleW[Ytr2== 3] = np.ceil(size0/size3)\n","\n","print(sampleW.shape)\n","\n","print(size1,size2,size3,size0)\n","\n","\n","print(weights)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(198, 128, 128, 4)\n","(198, 128, 128)\n","202494 59580 630383 2351575\n","{'0': 1.0, '1': 12.0, '2': 40.0, '3': 4.0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_PTUveA8uz5v","colab_type":"code","colab":{}},"source":["#adding the Dice coefficient \n","from keras import backend as K\n","\n","def loss_tot(y_true, y_pred, smooth=1):\n","    \"\"\"\n","    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n","         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n","    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n","    \"\"\"\n","    \n","    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n","    dice = (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n","    return 1-dice+categorical_crossentropy(y_true, y_pred)                    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6iv6HCVrFc4","colab_type":"code","outputId":"ff9eb624-908f-4a3e-8027-54ca11b9b6cd","executionInfo":{"status":"ok","timestamp":1558093936147,"user_tz":-120,"elapsed":526,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":1499}},"source":["from keras.optimizers import Adam\n","#Load an existing model\n","model_name = 'inversenet.h5'\n","load_existant = False\n","#loss_weights = [0.5, 0.5]\n","\n","if load_existant:\n","  model = load_model(model_name) \n","  \n","else:\n","\n","  model = Model(inputs=first_input, outputs=output)\n","  model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0), loss=loss_tot, metrics=['acc'])\n","  model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0517 11:52:16.792664 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 128, 128, 1)  4           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 128, 128, 128 1280        batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 128, 128, 128 147584      conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 128, 128, 128 0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 127, 127, 128 0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 64, 64, 64)   73792       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 64, 64, 64)   0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 63, 63, 64)   0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 32)   18464       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 32)   9248        conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 32, 32, 32)   0           conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 31, 31, 32)   0           dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 16)   4624        max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 16)   2320        conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 16, 16, 16)   0           conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 16)   0           dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 32)   4640        up_sampling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 32, 32, 64)   0           dropout_3[0][0]                  \n","                                                                 conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 32)   18464       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 32, 32, 32)   0           conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 32)   0           dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 64, 64, 64)   18496       up_sampling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 64, 64, 128)  0           dropout_2[0][0]                  \n","                                                                 conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 64, 64, 64)   0           conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 64) 0           dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 128, 128, 128 73856       up_sampling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 128, 128, 256 0           dropout_1[0][0]                  \n","                                                                 conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 128, 128, 128 295040      concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 128, 128, 128 147584      conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 128, 128, 128 0           conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 128, 128, 128 512         dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 128, 128, 4)  516         batch_normalization_2[0][0]      \n","==================================================================================================\n","Total params: 973,320\n","Trainable params: 973,062\n","Non-trainable params: 258\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h7CYFbm8-rmH","colab_type":"code","colab":{}},"source":["#data augmentation \n","\n","# we create two instances with the same arguments\n","data_gen_args = dict(zoom_range=0.01, horizontal_flip=True, vertical_flip = True, rotation_range = 10)\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen = ImageDataGenerator(**data_gen_args)\n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","#image should have rank 4\n","image_datagen.fit(Xtr, augment=True, seed=seed)\n","mask_datagen.fit(Ytr, augment=True, seed=seed)\n","\n","image_generator = image_datagen.flow(Xtr,seed=seed)\n","\n","mask_generator = mask_datagen.flow(Ytr,seed=seed)\n","\n","# combine generators into one which yields image and masks\n","train_generator = zip(image_generator, mask_generator)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJDSiF3_K-SF","colab_type":"code","outputId":"c71b264c-16a8-4aa5-d366-f45458192574","executionInfo":{"status":"ok","timestamp":1558093966691,"user_tz":-120,"elapsed":17757,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#Apply early stopping\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","#https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6\n","tbc=TensorBoardColab()\n","early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience = 20, verbose=0)\n","check_point = ModelCheckpoint('gdrive/My Drive/Deep Learning Project/best_model_100epochs.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","https://111b11ba.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HtjpLlhOfNXm","colab_type":"code","outputId":"8f6cec74-c8a1-421b-9f60-9f015772b508","executionInfo":{"status":"ok","timestamp":1558094546805,"user_tz":-120,"elapsed":554255,"user":{"displayName":"Mathilde Aubret","photoUrl":"","userId":"10939187735756654348"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["# Train the model, iterating on the data in batches of 50 samples\n","#model.fit(Xtr, Ytr, epochs=100, batch_size=50)\n","history = model.fit_generator(train_generator, validation_data = (Xva, Yva), verbose = 1, steps_per_epoch=10, epochs=1, callbacks=[early_stopping,check_point,TensorBoardColabCallback(tbc)], class_weight = 'auto') #steps_per_epoch = data_size/n_batch\n","\n","path_new_model = \"gdrive/My Drive/Deep Learning Project/100epochs.h5\"\n","#history = model.fit(Xtr, Ytr, validation_data = (Xva, Yva), epochs=100, verbose = 1, batch_size=25, callbacks=[early_stopping, check_point,TensorBoardColabCallback(tbc)])\n","model.save(path_new_model)\n","\n","\n","#https://stackoverflow.com/questions/41651628/negative-dimension-size-caused-by-subtracting-3-from-1-for-conv2d"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0517 11:53:14.802436 139964132034432 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0517 11:53:16.680393 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","W0517 11:53:17.385306 139964132034432 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","10/10 [==============================] - 546s 55s/step - loss: 1.3884 - acc: 0.3096 - val_loss: 1.0577 - val_acc: 0.1900\n","\n","Epoch 00001: val_acc improved from -inf to 0.18997, saving model to gdrive/My Drive/Deep Learning Project/best_model_100epochs.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YjeV8DbusfaH","colab_type":"code","colab":{}},"source":["# list all data in history\n","print(history.history.keys())\n","# summarize history for accuracy\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuBQgwwEtdp7","colab_type":"code","outputId":"c3b2a36b-4db2-4d5e-c673-a4b7245e7fdb","executionInfo":{"status":"ok","timestamp":1558028342234,"user_tz":-120,"elapsed":543,"user":{"displayName":"Blanca Cabrera","photoUrl":"","userId":"05921244390916268848"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(history.history['acc'])\n","print(history.history['loss'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.3123565468252922, 0.4008436136099757, 0.4273301311838093]\n","[1.447887818829543, 0.9669403239172332, 0.8835473718927868]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o51Bthn3KuPe","colab_type":"code","colab":{}},"source":["# load the saved model\n","saved_model = load_model('gdrive/My Drive/Deep Learning Project/best_model.h5', custom_objects={'loss_tot': loss_tot})\n","# evaluate the model\n","train_loss, train_acc = saved_model.evaluate(Xtr, Ytr, verbose=0)\n","val_loss, val_acc = saved_model.evaluate(Xva, Yva, verbose=0)\n","\n","print('Training metrics: Loss {0} Accuracy: {1}'.format(train_loss,train_acc))\n","print('Validation metrics: Loss {0} Accuracy: {1}'.format(val_loss,val_acc))\n","\n","#https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bE5T9vkeIkHQ","colab_type":"code","colab":{}},"source":["#Test metrics\n","test_loss, test_acc = saved_model.evaluate(Xtst, Ytst, verbose=0)\n","print('Test metrics: Loss {0} Accuracy: {1}'.format(test_loss,test_acc))\n","\n","#Test predictions\n","prediction = model.predict(Xtst)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLUTOMs3BU3W","colab_type":"code","outputId":"850826d5-5f81-4072-fa70-0ace25629f3a","executionInfo":{"status":"ok","timestamp":1558016517565,"user_tz":-120,"elapsed":566,"user":{"displayName":"Blanca Cabrera","photoUrl":"","userId":"05921244390916268848"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(prediction.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(9, 128, 128, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HMCvlIg_KBmz","colab_type":"code","colab":{}},"source":["not_onehot_pred= np.argmax(prediction[1],2)\n","print(not_onehot_pred.shape)\n","plt.imshow(not_onehot_pred)\n","plt.colorbar()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tg7YIiMfVSZ","colab_type":"code","outputId":"9a914f5d-ce1e-40a6-ec39-4055d28dfb6b","executionInfo":{"status":"ok","timestamp":1558027368436,"user_tz":-120,"elapsed":13019,"user":{"displayName":"Blanca Cabrera","photoUrl":"","userId":"05921244390916268848"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Obtain the performance\n","score = model.evaluate(Xtst, Ytst, batch_size=10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r25/25 [==============================] - 12s 491ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7HOouGpKTF2f","colab_type":"code","outputId":"fcd0e328-e4cd-4e96-e9cb-e8e4a35e6834","executionInfo":{"status":"ok","timestamp":1558027379578,"user_tz":-120,"elapsed":685,"user":{"displayName":"Blanca Cabrera","photoUrl":"","userId":"05921244390916268848"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.8894362449645996, 0.4095825254917145]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DE2kvgUDqyBv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}